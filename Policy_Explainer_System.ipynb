{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a461361-9498-48ca-be66-2a8a1a912a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ac0b3a-bf37-413d-afee-e6f7089cd3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key:  ········\n"
     ]
    }
   ],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee675273-34d1-484c-a521-acc12e97ac0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30743/3789767084.py:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  chroma_vs = Chroma(persist_directory=persist_directory, embedding_function=OpenAIEmbeddings())\n"
     ]
    }
   ],
   "source": [
    "persist_directory = './persist'\n",
    "\n",
    "# Load the Chroma vector store\n",
    "chroma_vs = Chroma(persist_directory=persist_directory, embedding_function=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe585e63-060b-4d21-8119-00c7fa6ba10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdcddeb1-45f3-4d7d-850c-58632884b5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's the maternity leave policy?\"\n",
    "\n",
    "results = chroma_vs.similarity_search(query=query)\n",
    "\n",
    "for result in results:\n",
    "    print(result.page_content+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad8b714c-e449-4278-aab3-1e659775e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(messages):\n",
    "    \"\"\"\n",
    "    Generate a response using the chat completions API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09cb2e53-159d-41eb-bd29-67110f29a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_message(messages, role, content):\n",
    "    \"\"\"Append a message to the conversation history.\"\"\"\n",
    "    messages.append({\"role\": role, \"content\": content})\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ceb9b95-c5bc-4de6-a57c-72b551af1b6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Obtain answer from RAG\u001b[39;00m\n\u001b[1;32m      2\u001b[0m rag_results \u001b[38;5;241m=\u001b[39m chroma_vs\u001b[38;5;241m.\u001b[39msimilarity_search(query\u001b[38;5;241m=\u001b[39mquery)\n\u001b[0;32m----> 3\u001b[0m rag_result \u001b[38;5;241m=\u001b[39m \u001b[43mrag_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpage_content\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize conversation\u001b[39;00m\n\u001b[1;32m      6\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are an assistant summarizing policies into employee-friendly explanations.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      8\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: rag_result}\n\u001b[1;32m      9\u001b[0m ]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Obtain answer from RAG\n",
    "rag_results = chroma_vs.similarity_search(query=query)\n",
    "rag_result = rag_results[0].page_content\n",
    "\n",
    "# Initialize conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an assistant summarizing policies into employee-friendly explanations.\"},\n",
    "    {\"role\": \"user\", \"content\": rag_result}\n",
    "]\n",
    "\n",
    "response = get_response(messages)\n",
    "messages = append_message(messages, \"assistant\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f97c1c-f770-4a63-9c4e-2e3ef5455ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e2aa3-f6f8-44e1-9173-88cbc2d42c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea253d-50f2-4e1a-aa60-5606de5579f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = \"What if I am adopting a child?\"\n",
    "messages = append_message(messages, \"user\", query_2)\n",
    "get_response(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713d3f2a-d4e7-48a1-b015-9b10d599b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_results_aux = chroma_vs.similarity_search(query=query_2)\n",
    "rag_results_aux[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d8008-5feb-4b48-9a9e-1ac4d9fcb115",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = \"What if I am adopting a child?\"\n",
    "query_2 += \"\\n\" + rag_results_aux[0].page_content\n",
    "\n",
    "\n",
    "messages = append_message(messages, \"user\", query_2)\n",
    "get_response(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e534d93-8f5c-4b6a-af86-4a2d2176b9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66061e96-583b-44fa-89f3-b2a05a9e37dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733ef4b-0fc8-49dd-b924-3b6929d463ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b79b50-eeb2-47fa-9dfb-4e05d2231e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an assistant summarizing policies into employee-friendly explanations.\"}\n",
    "]\n",
    "while True:\n",
    "    # Let the user ask a question\n",
    "    query = input()\n",
    "\n",
    "    # Get the most similar answer from the RAG\n",
    "    rag_results = chroma_vs.similarity_search(query=query)\n",
    "    rag_result = rag_results[0].page_content # Get the most similar one\n",
    "\n",
    "    # Concatenate the user query with the rag response\n",
    "    final_query = query + \"\\n\" + rag_result\n",
    "\n",
    "    # Append user message\n",
    "    messages = append_message(messages, \"user\", final_query)\n",
    "\n",
    "    # Get response from the chatbot and add it to the message to continue the context\n",
    "    response = get_response(messages)\n",
    "    messages = append_message(messages, \"assistant\", response)\n",
    "    print(response+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b28e6-6ed1-4a1c-93b2-884b0e4f1234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e33119-b77b-430b-9b54-e2ce105b496c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56fa782-2bdc-4488-a76d-e1a195a912fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STARTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a0baa87-7f28-4f1b-8d6c-2dd03f84f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the collection\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_chroma_collection(name: str, directory: str) -> Chroma:\n",
    "    \"\"\"\n",
    "    Load an existing Chroma collection.\n",
    "\n",
    "    Args:\n",
    "        name (str): Name of the collection.\n",
    "        directory (str): Directory where the collection is persisted.\n",
    "\n",
    "    Returns:\n",
    "        Chroma: The loaded Chroma vectorstore.\n",
    "    \"\"\"\n",
    "    persist_directory = os.path.join(directory, name)\n",
    "    if not os.path.exists(persist_directory):\n",
    "        raise ValueError(f\"Collection '{name}' does not exist in '{directory}'.\")\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    collection = Chroma(\n",
    "        collection_name=name,\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    return collection\n",
    "\n",
    "\n",
    "# Load retriever from the collection\n",
    "def load_retriever_from_collection(\n",
    "    collection_name: str,\n",
    "    search_type: str = \"similarity_score_threshold\",\n",
    "    score_threshold: float = 0.3,\n",
    "    top_k: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a retriever from a Chroma collection with configurable retrieval behavior.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): Name of the Chroma collection.\n",
    "        search_type (str): Retrieval type (similarity_score_threshold or mmr).\n",
    "        score_threshold (float): Minimum similarity score for retrieval.\n",
    "        top_k (int): Number of documents to return.\n",
    "\n",
    "    Returns:\n",
    "        Retriever: Configured retriever.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the persisted collection\n",
    "    collection = load_chroma_collection(name=collection_name, directory=\"./persist\")\n",
    "    \n",
    "    # Build retriever with configurable behavior\n",
    "    retriever = collection.as_retriever(\n",
    "        search_type=search_type,\n",
    "        search_kwargs={\n",
    "            \"score_threshold\": score_threshold,\n",
    "            \"k\": top_k\n",
    "        }\n",
    "    )\n",
    "    return retriever\n",
    "\n",
    "# load retirever with metadata filtering\n",
    "def load_retriever_with_metadata_from_collection(\n",
    "    collection_name: str,\n",
    "    search_type: str = \"similarity_score_threshold\",\n",
    "    score_threshold: float = 0.3,\n",
    "    top_k: int = 5,\n",
    "    metadata_filter: dict = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a retriever from a Chroma collection with configurable retrieval behavior\n",
    "    and optional metadata filtering.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): Name of the Chroma collection.\n",
    "        search_type (str): Retrieval type (\"similarity_score_threshold\" or \"mmr\").\n",
    "        score_threshold (float): Minimum similarity score for retrieval.\n",
    "        top_k (int): Number of documents to return.\n",
    "        metadata_filter (dict): Optional filter, e.g. {\"source\": \"assets/documents/vacation-policy.pdf\"}\n",
    "\n",
    "    Returns:\n",
    "        Retriever: Configured retriever.\n",
    "    \"\"\"\n",
    "    collection = load_chroma_collection(name=collection_name, directory=\"./persist\")\n",
    "    \n",
    "    retriever = collection.as_retriever(\n",
    "        search_type=search_type,\n",
    "        search_kwargs={\n",
    "            \"score_threshold\": score_threshold,\n",
    "            \"k\": top_k,\n",
    "            \"filter\": metadata_filter  # <-- apply metadata filter\n",
    "        }\n",
    "    )\n",
    "    return retriever\n",
    "\n",
    "\n",
    "# Retrieve with expanded queries\n",
    "def retrieve_with_expanded_queries(\n",
    "    collection_name: str,\n",
    "    queries: List[str],\n",
    "    search_type: str = \"similarity_score_threshold\",\n",
    "    score_threshold: float = 0.3,\n",
    "    top_k: int = 5,\n",
    "    metadata_filter: dict = None\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents from a Chroma collection using one or more expanded queries.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): Name of the Chroma collection.\n",
    "        queries (List[str]): List of queries, e.g., original query + expanded terms.\n",
    "        search_type (str): Retrieval type (\"similarity_score_threshold\" or \"mmr\").\n",
    "        score_threshold (float): Minimum similarity score.\n",
    "        top_k (int): Number of documents to return per query.\n",
    "        metadata_filter (dict): Optional metadata filter.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: Aggregated, deduplicated documents.\n",
    "    \"\"\"\n",
    "    retriever = load_retriever_from_collection(\n",
    "        collection_name=collection_name,\n",
    "        search_type=search_type,\n",
    "        score_threshold=score_threshold,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for q in queries:\n",
    "        docs = retriever.get_relevant_documents(q)\n",
    "        results.extend(docs)\n",
    "    \n",
    "    # Deduplicate by source or content\n",
    "    unique_results = {d.metadata.get(\"source\", d.page_content): d for d in results}\n",
    "    return list(unique_results.values())\n",
    "\n",
    "# Simple query expansion function\n",
    "def expand_query(query: str, n_terms: int = 5) -> list[str]: \n",
    "    \"\"\" Use LLM to generate related terms for query expansion. \"\"\" \n",
    "    client = OpenAI() \n",
    "    prompt = f\"\"\" \n",
    "        Generate {n_terms} synonyms of the core word/phrase of the following query for use in document retrieval. \n",
    "        Keep them short, noun-phrases. Query: \"{query}\" \"\"\" \n",
    "    \n",
    "    response = client.chat.completions.create( model=\"gpt-4o-mini\", messages=[{\"role\":\"user\",\"content\": prompt}], max_tokens=100 ) \n",
    "    text = response.choices[0].message.content.strip() \n",
    "    \n",
    "    return [t.strip(\"-• \") for t in text.split(\"\\n\") if t.strip()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d00993c3-a80f-400b-a8af-d7c10c9b13f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "of paid maternity leave, while non-birth parents receive six weeks of paid paternity leave. adoptive parents receive eight weeks of paid leave that can be shared between both parents. employees must have been with the company for at least 12 months to qualify for paid parental leave, though unpaid l ...\n",
      "Metadata: {'source': 'assets/documents/childcare-policy.pdf'}\n",
      "\n",
      "Result 2:\n",
      "for ﬁnding specialized care providers in the community. this policy is eﬀective as of [current date] and may be modiﬁed as business needs and legal requirements change. employees will receive 30 days advance notice of any signiﬁcant changes to childcare beneﬁts. for speciﬁc questions about your situ ...\n",
      "Metadata: {'source': 'assets/documents/childcare-policy.pdf'}\n",
      "\n",
      "Result 3:\n",
      "launch periods, and other critical business periods that will be communicated to employees at least 60 days in advance. while we try to minimize blackout periods, these restrictions help ensure we can meet our commitments to clients and stakeholders during crucial times. year-end and carryover polic ...\n",
      "Metadata: {'source': 'assets/documents/vacation-policy.pdf'}\n"
     ]
    }
   ],
   "source": [
    "query = \"Maternity leave policy\"\n",
    "expanded_terms = expand_query(query, n_terms=3)\n",
    "all_queries = [query] + expanded_terms\n",
    "\n",
    "retriever = load_retriever_from_collection(\n",
    "    collection_name=\"benefits_collection\",\n",
    "    score_threshold=0.6,\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(doc.page_content[:300], \"...\")\n",
    "    print(\"Metadata:\", doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe650c86-462c-4c2b-81c6-67ce2c95d289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Maternity leave policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Maternity Leave Policy Summary**\n",
      "\n",
      "At our company, we offer supportive parental leave options to help new parents transition during this important time.\n",
      "\n",
      "- **Paid Leave**: \n",
      "  - **Birth Parents**: Eligible employees can take 12 weeks of paid maternity leave.\n",
      "  - **Non-Birth Parents**: These employees are entitled to six weeks of paid paternity leave.\n",
      "  - **Adoptive Parents**: They can receive eight weeks of paid leave, which can be shared between partners.\n",
      "\n",
      "- **Eligibility**: To qualify for paid leave, you must have been with the company for at least 12 months. However, if you've been with us for less time, you may have options for unpaid leave under FMLA guidelines.\n",
      "\n",
      "- **Benefits During Leave**: Your health insurance and other benefits remain active while you're on leave, and we guarantee you a spot in our on-site childcare center when you return.\n",
      "\n",
      "- **Return-to-Work Flexibility**: We understand the transition back to work can be challenging, so we offer flexible return-to-work arrangements. You can opt for part-time schedules for the first 4-6 weeks or take advantage of extended work-from-home options.\n",
      "\n",
      "- **Support for Nursing Mothers**: We have four dedicated lactation rooms available, equipped with hospital-grade pumps and comfortable seating. You can reserve these rooms through our online system to ensure privacy and availability.\n",
      "\n",
      "We are committed to supporting our employees during this significant life event, and we encourage you to reach out with any questions or for additional support.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what if I want to adopt a child?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Adoption Support Policy Summary**\n",
      "\n",
      "If you are considering adopting a child, our company is here to support you through the process. Here’s what you need to know:\n",
      "\n",
      "- **Adoption Assistance**: We provide up to $5,000 in reimbursement for adoption-related expenses. This can help cover costs like agency fees and legal services.\n",
      "\n",
      "- **Referrals**: We offer referrals to attorneys who specialize in adoption, ensuring you have access to knowledgeable support during the legal aspects of the process.\n",
      "\n",
      "- **Travel for International Adoptions**: If you’re adopting internationally, you may be eligible for additional unpaid leave for necessary travel. We also offer limited reimbursement for travel expenses associated with the adoption.\n",
      "\n",
      "- **Support for Nursing Mothers**: For those who may be nursing after bringing a child home, we have dedicated lactation rooms equipped with hospital-grade pumps, refrigeration, and comfortable seating. These rooms can be reserved online to ensure privacy.\n",
      "\n",
      "- **Flexible Schedules**: We understand the need for flexibility, so we accommodate pumping schedules and can adjust meeting times as needed to support nursing mothers.\n",
      "\n",
      "- **Special Needs Accommodation**: For employees with children who have special needs, our on-site childcare center is equipped to accommodate children with various developmental and physical disabilities. Our staff is trained to work with families to create individualized care plans, coordinate with external therapy providers, and help families access specialized services.\n",
      "\n",
      "Our company is committed to supporting you through your adoption journey and ensuring a smooth transition as you welcome your new child into your family. If you have any questions or need further assistance, don’t hesitate to reach out.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " What if I adopt a dog?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Pet Adoption Support Policy Summary**\n",
      "\n",
      "While our company has comprehensive support for human adoptions, we understand that bringing a pet into your family is also a significant commitment. Here's how we are here to support you if you adopt a dog:\n",
      "\n",
      "- **Pet Ownership Support**: While we don't offer specific reimbursement for pet adoption expenses like we do for human adoptions, we encourage our employees to take advantage of our flexible work schedules to help acclimate their new pet to their home environment.\n",
      "\n",
      "- **Flexible Work Arrangements**: If you're adopting a dog, you may consider utilizing hybrid work options or flexible hours to make the transition smoother for both you and your new pet.\n",
      "\n",
      "- **Pet-Friendly Culture**: We aim to foster a pet-friendly workplace culture. Check with your manager about policies regarding pets in the office or any pet-related events our company may host.\n",
      "\n",
      "We are committed to promoting a healthy work-life balance and understanding the joys and responsibilities that come with pet ownership. If you have further questions or suggestions about our support for pet owners, feel free to reach out!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an assistant summarizing policies into employee-friendly explanations.\"}\n",
    "]\n",
    "while True:\n",
    "    # Let the user ask a question\n",
    "    query = input()\n",
    "\n",
    "    # Get the most similar answer from the RAG\n",
    "    rag_results = retriever.get_relevant_documents(query)\n",
    "    rag_result = rag_results[0].page_content # Get the most similar one\n",
    "\n",
    "    # Concatenate the user query with the rag response\n",
    "    final_query = query + \"\\n\" + rag_result\n",
    "\n",
    "    # Append user message\n",
    "    messages = append_message(messages, \"user\", final_query)\n",
    "\n",
    "    # Get response from the chatbot and add it to the message to continue the context\n",
    "    response = get_response(messages)\n",
    "    messages = append_message(messages, \"assistant\", response)\n",
    "    print(response+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c8f9e5-46d8-4853-85f5-e9493cf3e1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
