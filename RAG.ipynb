{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bac5472",
   "metadata": {},
   "source": [
    "## Policy Document Indexing for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5fb6d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielazemencikova/Desktop/capstone/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import string\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Path to the documents\n",
    "path = 'assets/documents'\n",
    "\n",
    "# Show all files in the folder:\n",
    "files = [f for f in os.listdir(path) if f.lower().endswith('.pdf')]\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Function to extract text from a PDF file\n",
    "def extract_text_from_pdf(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts all text from a PDF file using PyPDF2.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted raw text.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    reader = PdfReader(file_path)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() or \"\"  # handle None if empty page\n",
    "    return text\n",
    "\n",
    "# We start by splitting the document into sections for later text preprocessing\n",
    "def split_into_sections(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Splits text into sections based on detected headings.\n",
    "    Returns a dictionary {heading: content}.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    sections = {}\n",
    "    current_heading = \"Document\"\n",
    "    current_content = []\n",
    "    \n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        # Heuristic: heading if short, capitalized, and not ending with period\n",
    "        if stripped and len(stripped.split()) <= 6 and stripped[0].isupper() and not stripped.endswith('.'):\n",
    "            # save previous section\n",
    "            if current_content:\n",
    "                sections[current_heading] = \" \".join(current_content).strip()\n",
    "            # start new section\n",
    "            current_heading = stripped\n",
    "            current_content = []\n",
    "        else:\n",
    "            current_content.append(stripped)\n",
    "    \n",
    "    # save last section\n",
    "    if current_content:\n",
    "        sections[current_heading] = \" \".join(current_content).strip()\n",
    "    \n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05617f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # number of characters per chunk\n",
    "    chunk_overlap=50  # overlap to maintain context\n",
    ")\n",
    "\n",
    "all_chunks = []\n",
    "all_metadatas = []\n",
    "\n",
    "for file in files:\n",
    "    pdf_path = os.path.join(path, file)\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    sections = split_into_sections(pdf_text)\n",
    "    \n",
    "    for section_title, content in sections.items():\n",
    "        chunks = text_splitter.split_text(content)\n",
    "        all_chunks.extend(chunks)\n",
    "        \n",
    "        # Add metadata per chunk, including section\n",
    "        for _ in chunks:\n",
    "            all_metadatas.append({\"source\": file, \"section\": section_title})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2585646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"./persist\"\n",
    "\n",
    "# # Remove old data\n",
    "# if os.path.exists(persist_directory):\n",
    "#     shutil.rmtree(persist_directory)\n",
    "\n",
    "# Create folder with proper permissions\n",
    "os.makedirs(persist_directory, exist_ok=True)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "vectorstore = Chroma.from_texts(\n",
    "                                all_chunks, \n",
    "                                embedding=embeddings, \n",
    "                                metadatas=all_metadatas,\n",
    "                                persist_directory=persist_directory\n",
    "                                )\n",
    "\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f20c54a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What's the maternity leave policy?\n",
      "\n",
      "Result 1 (from childcare-policy.pdf, section: Support for Working Parents):\n",
      "TechLance’s parental leave policies integrate closely with our childcare support programs. New mothers receive 12 weeks of paid maternity leave, while non-birth parents receive six weeks of paid paternity leave. Adoptive parents receive eight weeks of paid leave that can be shared between both parents. Employees must have been with the company for at least 12 months to qualify for paid parental leave, though unpaid leave options may be available for newer employees under FMLA guidelines.During\n",
      "\n",
      "Result 2 (from childcare-policy.pdf, section: Support for Working Parents):\n",
      "for newer employees under FMLA guidelines.During parental leave, health insurance and other beneﬁts continue, and we guarantee a spot in our on-site childcare center for employees returning from maternity or paternity leave. We also oﬀer ﬂexible return-to-work arrangements, including part-time schedules for the ﬁrst 4-6 weeks and extended work-from-home options to help ease the transition back to work. For nursing mothers, TechLance provides four dedicated lactation rooms equipped with\n",
      "\n",
      "Result 3 (from vacation-policy.pdf, section: Introduction):\n",
      "This policy applies to all regular full-time and part-time employees and outlines how vacation time is earned, scheduled, and used. We encourage all employees to plan and take their vacation time throughout the year rather than allowing it to accumulate, as this approach best serves both individual well-being and team productivity.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Query: What is the eligibility for Tuition Reimbursement\n",
      "\n",
      "Result 1 (from tuition-reimbursement-policy.pdf, section: Eligibility Requirements and Performance Standards):\n",
      "To be eligible for tuition reimbursement, employees must have completed at least 12 months of continuous full-time employment or 18 months of part-time employment (minimum 20 hours per week). This tenure requirement ensures that employees have established themselves in their roles and demonstrated commitment to TechLance before receiving educational beneﬁts. Performance standards are equally important in determining eligibility. Employees must maintain a performance rating of “Meets\n",
      "\n",
      "Result 2 (from tuition-reimbursement-policy.pdf, section: Service Commitments and Repayment Obligations):\n",
      "Receiving tuition reimbursement creates a service commitment to TechLance that varies based on the type of education completed. Undergraduate programs require an 18-month employment commitment after degree completion, graduate programs require 24 months, professional certiﬁcations require 12 months, and individual courses require six months per course.These commitments begin upon successful completion of the education, not when you receive reimbursement. Multiple courses or programs may result\n",
      "\n",
      "Result 3 (from tuition-reimbursement-policy.pdf, section: Approved Programs and Institutions):\n",
      "request consideration of other programs through the application process. HR can help you determine whether a speciﬁc program or institution would qualify for reimbursement before you begin the application process. This tuition reimbursement policy reﬂects TechLance’s commitment to employee development and lifelong learning. We believe that education beneﬁts both individual career growth and organizational capability, creating a win-win situation that strengthens our team and ourbusiness. We\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "queries = [\n",
    "    \"What's the maternity leave policy?\",\n",
    "    \"What is the eligibility for Tuition Reimbursement\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    results = vectorstore.similarity_search(q, k=3)\n",
    "    print(f\"Query: {q}\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(results):\n",
    "        source = doc.metadata.get(\"source\", \"unknown\")\n",
    "        section = doc.metadata.get(\"section\", \"unknown\")\n",
    "        print(f\"Result {i+1} (from {source}, section: {section}):\\n{doc.page_content}\\n\")\n",
    "    \n",
    "    print(\"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dfb981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92ea6c8f",
   "metadata": {},
   "source": [
    "### Create a function that will check last modified time of the files and if it is not new we won't need to re-create the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1dd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# assign an index to the files and store it together with the last modified date of the file\n",
    "file_index = []\n",
    "for idx, file in enumerate(files):\n",
    "    if file.lower().endswith('.pdf'):\n",
    "        file_path = os.path.join(path, file)\n",
    "        mod_time = os.path.getmtime(file_path)\n",
    "        file_index.append({\n",
    "            'index': idx,\n",
    "            'file': file,\n",
    "            'last_modified': datetime.fromtimestamp(mod_time)\n",
    "        })\n",
    "\n",
    "file_index_df = pd.DataFrame(file_index)\n",
    "print(file_index_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66662b0e",
   "metadata": {},
   "source": [
    "### Text Extraction, Cleaning, Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1aa425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that will use PyPDF2 library to extrac text from policy PDFs\n",
    "# Create a functions that will extracts pdf to text, remove the noise, split the text into section\n",
    "\n",
    "import re\n",
    "import string\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Function to extract text from a PDF file\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts all text from a PDF file using PyPDF2.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted raw text.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() or \"\"  # handle None if empty page\n",
    "    return text\n",
    "\n",
    "# We start by splitting the document into sections for later text preprocessing\n",
    "def split_into_sections(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Splits text into sections based on detected headings.\n",
    "    Returns a dictionary {heading: content}.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    sections = {}\n",
    "    current_heading = \"Document\"\n",
    "    current_content = []\n",
    "    \n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        # Heuristic: heading if short, capitalized, and not ending with period\n",
    "        if stripped and len(stripped.split()) <= 6 and stripped[0].isupper() and not stripped.endswith('.'):\n",
    "            # save previous section\n",
    "            if current_content:\n",
    "                sections[current_heading] = \" \".join(current_content).strip()\n",
    "            # start new section\n",
    "            current_heading = stripped\n",
    "            current_content = []\n",
    "        else:\n",
    "            current_content.append(stripped)\n",
    "    \n",
    "    # save last section\n",
    "    if current_content:\n",
    "        sections[current_heading] = \" \".join(current_content).strip()\n",
    "    \n",
    "    return sections\n",
    "\n",
    "# Function to clean and remove noise from text\n",
    "# We observe that the pdfs don't contain any page numbers, or images\n",
    "def clean_text(text: str, lowercase: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Cleans extracted PDF text for preprocessing:\n",
    "    - Lowercase (optional)\n",
    "    - Remove line breaks, tabs\n",
    "    - Remove punctuation\n",
    "    - Normalize spaces\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw extracted text.\n",
    "        lowercase (bool): Convert to lowercase (default True).\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text ready for NLP tasks.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase if needed\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Replace newlines and tabs with space\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "segments = []\n",
    "for file in tqdm(files):\n",
    "    if file.lower().endswith('.pdf'):\n",
    "        pdf_path = os.path.join(path, file)\n",
    "        raw_text = extract_text_from_pdf(pdf_path)\n",
    "        sections = split_into_sections(raw_text)\n",
    "\n",
    "        for heading, content in sections.items():\n",
    "            cleaned = clean_text(content)\n",
    "            segments.append({\n",
    "                'file': file,\n",
    "                'section': heading,\n",
    "                'text': cleaned\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame for easy processing\n",
    "segments_df = pd.DataFrame(segments)\n",
    "\n",
    "chroma_db_input = pd.merge(segments_df, file_index_df, on = 'file', how = 'left')\n",
    "\n",
    "# Create a unique index by combining file index and section order\n",
    "chroma_db_input['unique_id'] = chroma_db_input['index'].astype(str) + '_' + chroma_db_input.groupby('file').cumcount().astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da0a2d0",
   "metadata": {},
   "source": [
    "### We will check later if we need to split it into smaller chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3bd8b1",
   "metadata": {},
   "source": [
    "### Chroma collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0492e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b740170",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db_input['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a27927",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "persist_directory = \"./persist\"\n",
    "\n",
    "# Convert your segments DataFrame into Document objects\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=row['text'],\n",
    "        metadata={\n",
    "            'file': row['file'],\n",
    "            'section': row['section'],\n",
    "            'chunk_id': row['unique_id']\n",
    "        }\n",
    "    )\n",
    "    for _, row in chroma_db_input.iterrows()\n",
    "]\n",
    "\n",
    "# Create embeddings and Chroma vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "vectorstore.persist()\n",
    "print(\"Chroma vector store created and persisted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e69bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Chroma vector store using LangChain integration\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings  # or use another embedding model\n",
    "\n",
    "persist_directory = \"./persist\"\n",
    "\n",
    "# Prepare documents and metadatas\n",
    "documents = chroma_db_input[\"text\"].tolist()\n",
    "metadatas = chroma_db_input[[\"file\", \"section\", \"last_modified\", \"unique_id\"]].to_dict(orient=\"records\")\n",
    "ids = chroma_db_input[\"unique_id\"].tolist()\n",
    "\n",
    "# Initialize embeddings (replace with your preferred embedding model if needed)\n",
    "embedding_function = OpenAIEmbeddings()  \n",
    "\n",
    "# Create the Chroma vector store\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=documents,\n",
    "    embedding=embedding_function,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "vectordb.persist()\n",
    "print(\"Chroma vector store created and persisted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94b5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain chromadb PyPDF2 tiktoken openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9753519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8729ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac398c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b984c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77590f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
