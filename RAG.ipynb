{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bac5472",
   "metadata": {},
   "source": [
    "## Policy Document Indexing for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fb6d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "import string\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Path to the documents\n",
    "path = 'assets/documents'\n",
    "\n",
    "# Show all files in the folder:\n",
    "files = [f for f in os.listdir(path) if f.lower().endswith('.pdf')]\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to extract text from a PDF file\n",
    "def extract_text_from_pdf(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts all text from a PDF file using PyPDF2.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted raw text.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    reader = PdfReader(file_path)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() or \"\"  # handle None if empty page\n",
    "    return text\n",
    "\n",
    "# We start by splitting the document into sections for later text preprocessing\n",
    "def split_into_sections(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Splits text into sections based on detected headings.\n",
    "    Returns a dictionary {heading: content}.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    sections = {}\n",
    "    current_heading = \"Document\"\n",
    "    current_content = []\n",
    "    \n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        # Heuristic: heading if short, capitalized, and not ending with period\n",
    "        if stripped and len(stripped.split()) <= 6 and stripped[0].isupper() and not stripped.endswith('.'):\n",
    "            # save previous section\n",
    "            if current_content:\n",
    "                sections[current_heading] = \" \".join(current_content).strip()\n",
    "            # start new section\n",
    "            current_heading = stripped\n",
    "            current_content = []\n",
    "        else:\n",
    "            current_content.append(stripped)\n",
    "    \n",
    "    # save last section\n",
    "    if current_content:\n",
    "        sections[current_heading] = \" \".join(current_content).strip()\n",
    "    \n",
    "    return sections\n",
    "\n",
    "# Function to clean and remove noise from text\n",
    "# We observe that the pdfs don't contain any page numbers, or images\n",
    "def clean_text(text: str, lowercase: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Cleans extracted PDF text for preprocessing:\n",
    "    - Lowercase (optional)\n",
    "    - Remove line breaks, tabs\n",
    "    - Remove punctuation\n",
    "    - Normalize spaces\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw extracted text.\n",
    "        lowercase (bool): Convert to lowercase (default True).\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text ready for NLP tasks.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase if needed\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Replace newlines and tabs with space\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # number of characters per chunk\n",
    "    chunk_overlap=50  # overlap to maintain context\n",
    ")\n",
    "\n",
    "all_chunks = []\n",
    "all_metadatas = []\n",
    "\n",
    "for file in files:\n",
    "    pdf_path = os.path.join(path, file)\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    sections = split_into_sections(pdf_text)\n",
    "    \n",
    "    for section_title, content in sections.items():\n",
    "        chunks = text_splitter.split_text(content)\n",
    "        all_chunks.extend(chunks)\n",
    "        \n",
    "        # Add metadata per chunk, including section\n",
    "        for _ in chunks:\n",
    "            all_metadatas.append({\"source\": file, \"section\": section_title})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2585646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"./persist\"\n",
    "\n",
    "# # Remove old data\n",
    "# if os.path.exists(persist_directory):\n",
    "#     shutil.rmtree(persist_directory)\n",
    "\n",
    "# # Create folder with proper permissions\n",
    "# os.makedirs(persist_directory, exist_ok=True)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "vectorstore = Chroma.from_texts(\n",
    "                                all_chunks, \n",
    "                                embedding=embeddings, \n",
    "                                metadatas=all_metadatas,\n",
    "                                persist_directory=persist_directory\n",
    "                                )\n",
    "\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c54a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "queries = [\n",
    "    \"What's the maternity leave policy?\",\n",
    "    \"What is the eligibility for Tuition Reimbursement\",\n",
    "    \"How much can employees contribute to 401-k?\",\n",
    "    \"Do I have to manually enroll for 401-k?\",\n",
    "    \"I work in Finance, can I work remotely?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    results = vectorstore.similarity_search(q, k=3)\n",
    "    print(f\"Query: {q}\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(results):\n",
    "        source = doc.metadata.get(\"source\", \"unknown\")\n",
    "        section = doc.metadata.get(\"section\", \"unknown\")\n",
    "        print(f\"Result {i+1} (from {source}, section: {section}):\\n{doc.page_content}\\n\")\n",
    "    \n",
    "    print(\"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481f0b1",
   "metadata": {},
   "source": [
    "### Advanced RAG Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fdc76a",
   "metadata": {},
   "source": [
    "Metadata Filtering - Useful if we want to use only specific files for our answers or we want to search in specific section of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a337bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata Filtering\n",
    "query = \"What is the maternity leave policy?\"\n",
    "\n",
    "# Filter chunks where file = 'childcare-policy.pdf'\n",
    "results = vectorstore.similarity_search(\n",
    "    query, \n",
    "    k=3,\n",
    "    filter={\"source\": \"vacation-policy.pdf\"}  # Metadata filter\n",
    ")\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"{i+1}. {doc.metadata['source']} - {doc.metadata['section']}\")\n",
    "    print(doc.page_content, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e23db46",
   "metadata": {},
   "source": [
    "Query exapnsion - Automatically expand your query with related terms to improve retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Maternity leave policy\"\n",
    "\n",
    "# Simple query expansion (you could also use an LLM to generate expansions)\n",
    "expanded_terms = [\"parental leave\", \"pregnancy leave\", \"childcare leave\"]\n",
    "expanded_query = query + \", \" + \", \".join(expanded_terms)\n",
    "\n",
    "results = vectorstore.similarity_search(expanded_query, k=3)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"{i+1}. {doc.metadata['source']} - {doc.metadata['section']}\")\n",
    "    print(doc.page_content, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c6a3ca",
   "metadata": {},
   "source": [
    "HyDE - Generate a “hypothetical answer” for the query, then retrieve documents closest to that answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bef634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Step 1: Generate hypothetical answer\n",
    "prompt = f\"Generate a concise hypothetical answer to this question: '{query}'\"\n",
    "hypothetical_answer = llm.predict(prompt)\n",
    "\n",
    "# Step 2: Retrieve documents using embedding of the hypothetical answer\n",
    "embedding_fn = OpenAIEmbeddings()\n",
    "hypothetical_vector = embedding_fn.embed_query(hypothetical_answer)\n",
    "\n",
    "# Chroma supports querying via embedding directly\n",
    "results = vectorstore.similarity_search_by_vector(hypothetical_vector, k=3)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"{i+1}. {doc.metadata['source']} - {doc.metadata['section']}\")\n",
    "    print(doc.page_content, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dfb981",
   "metadata": {},
   "source": [
    "# Make the code more modualar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea6c8f",
   "metadata": {},
   "source": [
    "### Create a function that will check last modified time of the files and if it is not new we won't need to re-create the vector store\n",
    "Below there are functions that are included in ```rag.py``` - they check for the vector store and if it is up to date with pdf documents in a given folder. If no, the store is recreated. Furthermore, there is a processing of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5e10ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielazemencikova/Desktop/capstone/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "from datetime import datetime\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env\n",
    "\n",
    "class PDFProcessor:\n",
    "    \"\"\"\n",
    "    Processes PDF documents:\n",
    "    - Extracts raw text\n",
    "    - Splits text into sections based on headings\n",
    "    - Cleans text for NLP\n",
    "    - Splits text into chunks with metadata\n",
    "    \"\"\"\n",
    "    def __init__(self, pdf_folder: str, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "        self.pdf_folder = pdf_folder\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap\n",
    "        )\n",
    "        self.files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "    def extract_text(self, file_path: str) -> str:\n",
    "        \"\"\"Extracts raw text from a PDF using PyPDF2.\"\"\"\n",
    "        text = \"\"\n",
    "        reader = PdfReader(file_path)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "        return text\n",
    "\n",
    "    def split_into_sections(self, text: str) -> dict:\n",
    "        \"\"\"Splits text into sections based on detected headings.\"\"\"\n",
    "        lines = text.splitlines()\n",
    "        sections = {}\n",
    "        current_heading = \"Document\"\n",
    "        current_content = []\n",
    "\n",
    "        for line in lines:\n",
    "            stripped = line.strip()\n",
    "            if stripped and len(stripped.split()) <= 6 and stripped[0].isupper() and not stripped.endswith(\".\"):\n",
    "                if current_content:\n",
    "                    sections[current_heading] = \" \".join(current_content).strip()\n",
    "                current_heading = stripped\n",
    "                current_content = []\n",
    "            else:\n",
    "                current_content.append(stripped)\n",
    "\n",
    "        if current_content:\n",
    "            sections[current_heading] = \" \".join(current_content).strip()\n",
    "\n",
    "        return sections\n",
    "\n",
    "    def clean_text(self, text: str, lowercase: bool = True) -> str:\n",
    "        \"\"\"Cleans text: lowercases, removes punctuation, normalizes spaces.\"\"\"\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text.strip()\n",
    "\n",
    "    def process_pdfs(self):\n",
    "        \"\"\"\n",
    "        Processes all PDFs in the folder:\n",
    "        - Extracts text\n",
    "        - Splits into sections\n",
    "        - Splits sections into chunks\n",
    "        - Returns chunks and metadata\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        all_metadatas = []\n",
    "\n",
    "        for file in self.files:\n",
    "            pdf_path = os.path.join(self.pdf_folder, file)\n",
    "            raw_text = self.extract_text(pdf_path)\n",
    "            sections = self.split_into_sections(raw_text)\n",
    "\n",
    "            for section_title, content in sections.items():\n",
    "                cleaned_content = self.clean_text(content)\n",
    "                chunks = self.text_splitter.split_text(cleaned_content)\n",
    "                all_chunks.extend(chunks)\n",
    "                all_metadatas.extend([{\"source\": file, \"section\": section_title}] * len(chunks))\n",
    "\n",
    "        return all_chunks, all_metadatas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31853c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store is up to date. Last updated: 2025-08-22 17:09:21.983432\n"
     ]
    }
   ],
   "source": [
    "# rag.py\n",
    "# Contains PDFProcessor and VectorStoreManager definitions\n",
    "\n",
    "# main.py\n",
    "from rag import PDFProcessor, VectorStoreManager\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PDF_FOLDER = \"assets/documents\"\n",
    "PERSIST_DIR = \"./persist\"\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "processor = PDFProcessor(pdf_folder=PDF_FOLDER)\n",
    "chunks, metadatas = processor.process_pdfs()\n",
    "\n",
    "manager = VectorStoreManager(\n",
    "    pdf_folder=PDF_FOLDER,\n",
    "    persist_dir=PERSIST_DIR,\n",
    "    embeddings=embeddings,\n",
    "    chunks=chunks,\n",
    "    metadatas=metadatas\n",
    ")\n",
    "\n",
    "vectorstore = manager.load_or_create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764622f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
