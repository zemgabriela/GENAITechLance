{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bac5472",
   "metadata": {},
   "source": [
    "## Policy Document Indexing for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c2a6a7",
   "metadata": {},
   "source": [
    "We start by creating the following functions: ```clean_text``` & ```load_files```. When looking at the given pdfs, we observe that the pdf is clean. There is no number of pages, images any extra context that should be removed. However we create a function that would lower the text, removes multiple spaces, etc. We decided not to get rid of punctions as they seem to be relevant. When it comes to the function ```load_files```, we assume that in the future the folder will be filled with other file extensions as well. Thus we create a generic function that would look at pdfs and other extensions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60d35916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded & cleaned tuition-reimbursement-policy.pdf\n",
      "Loaded & cleaned health-insurance-policy.pdf\n",
      "Loaded & cleaned work-from-home-policy.pdf\n",
      "Loaded & cleaned gym-policy.pdf\n",
      "Loaded & cleaned vacation-policy.pdf\n",
      "Loaded & cleaned 401k-retirement-policy.pdf\n",
      "Loaded & cleaned life-insurance-policy.pdf\n",
      "Loaded & cleaned childcare-policy.pdf\n",
      "\n",
      "Total loaded documents: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import TextLoader, Docx2txtLoader, CSVLoader\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Path to the documents\n",
    "dir_path = 'assets/documents/'\n",
    "\n",
    "# -------------------------------\n",
    "# Text cleaning function\n",
    "# -------------------------------\n",
    "# Function to clean and remove noise from text\n",
    "#Â We observe that the pdfs don't contain any page numbers, or images\n",
    "def clean_text(text: str, lowercase: bool = True, remove_punct: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Cleans extracted text for preprocessing:\n",
    "    - Lowercase (optional)\n",
    "    - Remove line breaks, tabs\n",
    "    - Remove punctuation (optional)\n",
    "    - Normalize spaces\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    # Replace newlines and tabs with space \n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    \n",
    "    if remove_punct:\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# -------------------------------\n",
    "# File loader\n",
    "# -------------------------------\n",
    "def load_files(path: str) -> list[Document]:\n",
    "    _, file_extension = os.path.splitext(path)\n",
    "    file_extension = file_extension.lower()\n",
    "\n",
    "    if file_extension == '.pdf':\n",
    "        reader = PdfReader(path)\n",
    "        all_text = \"\".join((p.extract_text() or \"\") for p in reader.pages)\n",
    "        cleaned = clean_text(all_text, lowercase=True, remove_punct=False)\n",
    "        return [Document(page_content=cleaned, metadata={\"source\": path})]\n",
    "\n",
    "    elif file_extension == '.txt':\n",
    "        docs = TextLoader(path, encoding='utf8').load()\n",
    "        for d in docs:\n",
    "            d.page_content = clean_text(d.page_content)\n",
    "        return docs\n",
    "\n",
    "    elif file_extension == '.docx':\n",
    "        docs = Docx2txtLoader(path).load()\n",
    "        for d in docs:\n",
    "            d.page_content = clean_text(d.page_content)\n",
    "        return docs\n",
    "        \n",
    "    elif file_extension == '.csv':\n",
    "        docs = CSVLoader(path).load()\n",
    "        for d in docs:\n",
    "            d.page_content = clean_text(d.page_content)\n",
    "        return docs\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Usage example\n",
    "# -------------------------------\n",
    "files = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
    "\n",
    "# Collect all loaded documents\n",
    "all_documents = []\n",
    "for filename in files:\n",
    "    full_path = os.path.join(dir_path, filename)\n",
    "    try:\n",
    "        docs = load_files(full_path)\n",
    "        all_documents.extend(docs)\n",
    "        print(f\"Loaded & cleaned {filename}\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "\n",
    "print(f\"\\nTotal loaded documents: {len(all_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd5768f",
   "metadata": {},
   "source": [
    "After loading and cleaning the documents, we split them into chunks. Firstly, we tried a function that would split the documents into section, to have another source of metadata - to refet to the document and a specific section, however if there are documents which have sections that are very long, that doesn't seem like a proper option. So we use ```RecursiveCharacterTextSplitter``` wuth chunk_overlap to keep the context between chunks and not lose meaning.\n",
    "\n",
    "Afterwards, we define a function ```create_chroma_collection``` that would create a vector store using openai embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We start by splitting the document into sections for later text preprocessing\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Suppose `documents` is what you loaded from load_files()\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,    # max characters per chunk\n",
    "    chunk_overlap=200,  # overlap between chunks (keeps context)\n",
    ")\n",
    "\n",
    "split_docs = splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"Original docs: {len(all_documents)}\")\n",
    "print(f\"Split docs: {len(split_docs)}\")\n",
    "\n",
    "# Show first 2 chunks\n",
    "for i, d in enumerate(split_docs[:2], 1):\n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(d.page_content[:300], \"...\")\n",
    "    print(\"Metadata:\", d.metadata)\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Create a new Chroma collection & split documents into chunks\n",
    "def create_chroma_collection(\n",
    "    name: str, \n",
    "    documents: List[Document], \n",
    "    directory: str\n",
    ") -> Chroma:\n",
    "    \"\"\"\n",
    "    Create or overwrite a Chroma collection with given documents.\n",
    "\n",
    "    Args:\n",
    "        name (str): Name of the collection.\n",
    "        documents (List[Document]): List of LangChain Document objects.\n",
    "        directory (str): Directory where the collection is persisted.\n",
    "\n",
    "    Returns:\n",
    "        Chroma: The created Chroma vectorstore.\n",
    "    \"\"\"\n",
    "    persist_directory = os.path.join(directory, name)\n",
    "    os.makedirs(persist_directory, exist_ok=True)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    # Create collection and persist it\n",
    "    collection = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        collection_name=name,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    collection.persist()\n",
    "    return collection\n",
    "\n",
    "collection = create_chroma_collection(\n",
    "    name=\"benefits_collection\",\n",
    "    documents=split_docs,\n",
    "    directory=\"./persist\"\n",
    ")\n",
    "\n",
    "print(\"Collection created and persisted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75693b82",
   "metadata": {},
   "source": [
    "Last functions to be created are ```load_chroma_collection```, ```add_documents_to_collection``` and ```load_retriever_from_collection```. \n",
    "\n",
    "With adding documents to the collection we use incremental updates. New PDFs, DOCX files, or CSVs may arrive over time. Instead of rebuilding the entire collection from scratch, we can add only the new documents. This saves time and computation, especially for large collections. Preserve embeddings for existing docs. Lastly, we can aggregate multiple new documents and add them in one go, improving efficiency.\n",
    "\n",
    "```load_retriever_from_collection``` helps not to recreate the existing vectorstore when restarting the script. The function had configurable retrieval parameters, where we can set things like score_threshold, search_type, or top_k when loading the retriever. This allows us to tune retrieval behavior without changing the underlying vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8047089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the collection\n",
    "def load_chroma_collection(name: str, directory: str) -> Chroma:\n",
    "    \"\"\"\n",
    "    Load an existing Chroma collection.\n",
    "\n",
    "    Args:\n",
    "        name (str): Name of the collection.\n",
    "        directory (str): Directory where the collection is persisted.\n",
    "\n",
    "    Returns:\n",
    "        Chroma: The loaded Chroma vectorstore.\n",
    "    \"\"\"\n",
    "    persist_directory = os.path.join(directory, name)\n",
    "    if not os.path.exists(persist_directory):\n",
    "        raise ValueError(f\"Collection '{name}' does not exist in '{directory}'.\")\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    collection = Chroma(\n",
    "        collection_name=name,\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    return collection\n",
    "\n",
    "# Add documents to the collection\n",
    "def add_documents_to_collection(collection: Chroma, new_documents: List[Document]) -> None:\n",
    "    \"\"\"\n",
    "    Add new documents to an existing Chroma collection.\n",
    "\n",
    "    Args:\n",
    "        collection (Chroma): The Chroma vectorstore to add documents to.\n",
    "        new_documents (List[Document]): List of new LangChain Document objects to add.\n",
    "    \"\"\"\n",
    "    if not new_documents:\n",
    "        print(\"No new documents to add.\")\n",
    "        return\n",
    "\n",
    "    collection.add_documents(new_documents)\n",
    "    collection.persist()\n",
    "    print(f\"Added {len(new_documents)} documents to the collection and persisted changes.\")\n",
    "    \n",
    "# Load retriever from the collection\n",
    "def load_retriever_from_collection(\n",
    "    collection_name: str,\n",
    "    search_type: str = \"similarity_score_threshold\",\n",
    "    score_threshold: float = 0.3,\n",
    "    top_k: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a retriever from a Chroma collection with configurable retrieval behavior.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): Name of the Chroma collection.\n",
    "        search_type (str): Retrieval type (similarity_score_threshold or mmr).\n",
    "        score_threshold (float): Minimum similarity score for retrieval.\n",
    "        top_k (int): Number of documents to return.\n",
    "\n",
    "    Returns:\n",
    "        Retriever: Configured retriever.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the persisted collection\n",
    "    collection = load_chroma_collection(name=collection_name, directory=\"./persist\")\n",
    "    \n",
    "    # Build retriever with configurable behavior\n",
    "    retriever = collection.as_retriever(\n",
    "        search_type=search_type,\n",
    "        search_kwargs={\n",
    "            \"score_threshold\": score_threshold,\n",
    "            \"k\": top_k\n",
    "        }\n",
    "    )\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351d1c1",
   "metadata": {},
   "source": [
    "Example of dynamically storing documents:\n",
    "The script will run only if there is a new file to be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bf8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing collection\n",
    "collection = load_chroma_collection(\"benefits_collection\", \"./persist\")\n",
    "\n",
    "# Load new PDFs\n",
    "new_docs = []\n",
    "new_files = [\"assets/new_policy.pdf\"]\n",
    "for f in new_files:\n",
    "    new_docs.extend(load_files(f))\n",
    "\n",
    "# Add to collection\n",
    "add_documents_to_collection(collection, new_docs)\n",
    "\n",
    "print(\"Collection updated with new documents!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "624f69d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Query: What's the maternity leave policy?\n",
      " Found 3 results\n",
      "\n",
      "--- Result 1 ---\n",
      "of paid maternity leave, while non-birth parents receive six weeks of paid paternity leave. adoptive parents receive eight weeks of paid leave that can be shared between both parents. employees must have been with the company for at least 12 months to qualify for paid parental leave, though unpaid l ...\n",
      "Metadata: {'source': 'assets/documents/childcare-policy.pdf'}\n",
      "\n",
      "--- Result 2 ---\n",
      "of paid maternity leave, while non-birth parents receive six weeks of paid paternity leave. adoptive parents receive eight weeks of paid leave that can be shared between both parents. employees must have been with the company for at least 12 months to qualify for paid parental leave, though unpaid l ...\n",
      "Metadata: {'source': 'assets/documents/childcare-policy.pdf'}\n",
      "\n",
      "--- Result 3 ---\n",
      "for ï¬nding specialized care providers in the community. this policy is eï¬ective as of [current date] and may be modiï¬ed as business needs and legal requirements change. employees will receive 30 days advance notice of any signiï¬cant changes to childcare beneï¬ts. for speciï¬c questions about your situ ...\n",
      "Metadata: {'source': 'assets/documents/childcare-policy.pdf'}\n",
      "\n",
      "\n",
      "Query: What is the eligibility for Tuition Reimbursement\n",
      " Found 3 results\n",
      "\n",
      "--- Result 1 ---\n",
      "standards to be eligible for tuition reimbursement, employees must have completed at least 12 months of continuous full-time employment or 18 months of part-time employment (minimum 20 hours per week). this tenure requirement ensures that employees have established themselves in their roles and demo ...\n",
      "Metadata: {'source': 'assets/documents/tuition-reimbursement-policy.pdf'}\n",
      "\n",
      "--- Result 2 ---\n",
      "to accommodate working professionals who want to advance their education while maintaining their career momentum. whether youâre pursuing your ï¬rst degree, advancing to graduate studies, or seeking professional certiï¬cations to enhance your expertise, techlance is here to support your educational jo ...\n",
      "Metadata: {'source': 'assets/documents/tuition-reimbursement-policy.pdf'}\n",
      "\n",
      "--- Result 3 ---\n",
      "separate check. reimbursements may be subject to income tax depending on irs regulations and the total amount received during the tax year. we recommend consulting with a tax professional if you have questions about the tax implications of educational reimbursements. service commitments and repaymen ...\n",
      "Metadata: {'source': 'assets/documents/tuition-reimbursement-policy.pdf'}\n",
      "\n",
      "\n",
      "Query: How much can employees contribute to 401-k?\n",
      " Found 3 results\n",
      "\n",
      "--- Result 1 ---\n",
      "50 and older can make additional âcatch-upâ contributions of $7,500 annually in 2024, allowing total contributions of up to $30,500. these catch-up contributions help employees who may have started saving for retirement later in their careers or who want to accelerate their savings as they approach  ...\n",
      "Metadata: {'source': 'assets/documents/401k-retirement-policy.pdf'}\n",
      "\n",
      "--- Result 2 ---\n",
      "accelerates your retirement savings, making it essential to contribute at least enough to receive the full company match. our matching contributions are subject to a vesting schedule that encourages long-term employment while protecting the companyâs investment in your retirement. youâre always 100% ...\n",
      "Metadata: {'source': 'assets/documents/401k-retirement-policy.pdf'}\n",
      "\n",
      "--- Result 3 ---\n",
      "and professionally managed investment options, you can create a solid foundation for retirement security while focusing on your career and current ï¬nancial needs. we believe that retirement planning should be a partnership between you and techlance, which is why we contribute signiï¬cantly to your re ...\n",
      "Metadata: {'source': 'assets/documents/401k-retirement-policy.pdf'}\n",
      "\n",
      "\n",
      "Query: Do I have to manually enroll for 401-k?\n",
      " Found 3 results\n",
      "\n",
      "--- Result 1 ---\n",
      "signiï¬cant company matching and establishes good savings habits from the beginning. if you prefer not to participate in the 401(k) plan, you can opt out during your ï¬rst 90 days and receive a refund of any contributions made. however, we strongly encourage participation since the combination of tax  ...\n",
      "Metadata: {'source': 'assets/documents/401k-retirement-policy.pdf'}\n",
      "\n",
      "--- Result 2 ---\n",
      "are designed to help you build substantial retirement savings without requiring constant attention or diï¬cult decisions. research shows that employees who participate in automatic enrollment and escalation typically accumulate signiï¬cantly more retirement wealth than those who donât, even when contr ...\n",
      "Metadata: {'source': 'assets/documents/401k-retirement-policy.pdf'}\n",
      "\n",
      "--- Result 3 ---\n",
      "accelerates your retirement savings, making it essential to contribute at least enough to receive the full company match. our matching contributions are subject to a vesting schedule that encourages long-term employment while protecting the companyâs investment in your retirement. youâre always 100% ...\n",
      "Metadata: {'source': 'assets/documents/401k-retirement-policy.pdf'}\n",
      "\n",
      "\n",
      "Query: I work in Finance, can I work remotely?\n",
      " Found 3 results\n",
      "\n",
      "--- Result 1 ---\n",
      "everyone involved. frequently asked questionscan i work from home occasionally without a formal remote work agreement? yes, you can work remotely up to two days per month with manager approval and advance notice for situations like appointments, weather issues, or home repairs. what happens if my in ...\n",
      "Metadata: {'source': 'assets/documents/work-from-home-policy.pdf'}\n",
      "\n",
      "--- Result 2 ---\n",
      "and oï¬ers reimbursement allowances for ergonomic improvements, internet service, and oï¬ce supplies. youâre responsible for basic furniture and utilities. what if my manager doesnât approve my remote work request? managers must provide speciï¬c business reasons for denial. you can discuss concerns wit ...\n",
      "Metadata: {'source': 'assets/documents/work-from-home-policy.pdf'}\n",
      "\n",
      "--- Result 3 ---\n",
      "home oï¬ce to protect company information. what happens if my performance declines while working remotely? your manager will work with you to identify issues and create an improvement plan, which may include additional support, training, or temporarily returning to on-site work. this policy is eï¬ecti ...\n",
      "Metadata: {'source': 'assets/documents/work-from-home-policy.pdf'}\n"
     ]
    }
   ],
   "source": [
    "retriever = load_retriever_from_collection(\"benefits_collection\", score_threshold = 0.6, top_k=3)\n",
    "\n",
    "\n",
    "queries = [\n",
    "    \"What's the maternity leave policy?\",\n",
    "    \"What is the eligibility for Tuition Reimbursement\",\n",
    "    \"How much can employees contribute to 401-k?\",\n",
    "    \"Do I have to manually enroll for 401-k?\",\n",
    "    \"I work in Finance, can I work remotely?\"\n",
    "]\n",
    "\n",
    "for i in queries:\n",
    "    print(f\"\\n\\nQuery: {i}\")\n",
    "    query = i\n",
    "    results = retriever.get_relevant_documents(query)\n",
    "\n",
    "    print(f\" Found {len(results)} results\")\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"\\n--- Result {i} ---\")\n",
    "        print(r.page_content[:300], \"...\")\n",
    "        print(\"Metadata:\", r.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2585646d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6518571c",
   "metadata": {},
   "source": [
    "## Try different types of retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e478f",
   "metadata": {},
   "source": [
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c54a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "queries = [\n",
    "    \"What's the maternity leave policy?\",\n",
    "    \"What is the eligibility for Tuition Reimbursement\",\n",
    "    \"How much can employees contribute to 401-k?\",\n",
    "    \"Do I have to manually enroll for 401-k?\",\n",
    "    \"I work in Finance, can I work remotely?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    results = vectorstore.similarity_search(q, k=3)\n",
    "    print(f\"Query: {q}\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(results):\n",
    "        source = doc.metadata.get(\"source\", \"unknown\")\n",
    "        section = doc.metadata.get(\"section\", \"unknown\")\n",
    "        print(f\"Result {i+1} (from {source}, section: {section}):\\n{doc.page_content}\\n\")\n",
    "    \n",
    "    print(\"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f17ed0",
   "metadata": {},
   "source": [
    "### Similarity search with score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a853835",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's the maternity leave policy?\"\n",
    "\n",
    "results = vectorstore.similarity_search_with_score(query, k=5)\n",
    "\n",
    "for i, (doc, score) in enumerate(results):\n",
    "    print(f\"\\n{i+1}. {doc.metadata['source']} - {doc.metadata['section']} (score={score:.4f})\")\n",
    "    print(doc.page_content[:300], \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61a3b3",
   "metadata": {},
   "source": [
    "### Max Marginal Relevance (MMR) search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc156d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorstore.max_marginal_relevance_search(query, k=5, fetch_k=15)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n{i+1}. {doc.metadata['source']} - {doc.metadata['section']}\")\n",
    "    print(doc.page_content[:300], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481f0b1",
   "metadata": {},
   "source": [
    "### Advanced RAG Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fdc76a",
   "metadata": {},
   "source": [
    "Metadata Filtering - Useful if we want to use only specific files for our answers or we want to search in specific section of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a337bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata Filtering\n",
    "query = \"What is the maternity leave policy?\"\n",
    "\n",
    "# Filter chunks where file = 'childcare-policy.pdf'\n",
    "results = vectorstore.similarity_search(\n",
    "    query, \n",
    "    k=3,\n",
    "    filter={\"source\": \"vacation-policy.pdf\"}  # Metadata filter\n",
    ")\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"{i+1}. {doc.metadata['source']} - {doc.metadata['section']}\")\n",
    "    print(doc.page_content, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e23db46",
   "metadata": {},
   "source": [
    "Query exapnsion - Automatically expand your query with related terms to improve retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Maternity leave policy\"\n",
    "\n",
    "# Simple query expansion (you could also use an LLM to generate expansions)\n",
    "expanded_terms = [\"parental leave\", \"pregnancy leave\", \"childcare leave\"]\n",
    "expanded_query = query + \", \" + \", \".join(expanded_terms)\n",
    "\n",
    "results = vectorstore.similarity_search(expanded_query, k=3)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"{i+1}. {doc.metadata['source']} - {doc.metadata['section']}\")\n",
    "    print(doc.page_content, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c6a3ca",
   "metadata": {},
   "source": [
    "HyDE - Generate a âhypothetical answerâ for the query, then retrieve documents closest to that answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bef634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Step 1: Generate hypothetical answer\n",
    "prompt = f\"Generate a concise hypothetical answer to this question: '{query}'\"\n",
    "hypothetical_answer = llm.predict(prompt)\n",
    "\n",
    "# Step 2: Retrieve documents using embedding of the hypothetical answer\n",
    "embedding_fn = OpenAIEmbeddings()\n",
    "hypothetical_vector = embedding_fn.embed_query(hypothetical_answer)\n",
    "\n",
    "# Chroma supports querying via embedding directly\n",
    "results = vectorstore.similarity_search_by_vector(hypothetical_vector, k=3)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"{i+1}. {doc.metadata['source']} - {doc.metadata['section']}\")\n",
    "    print(doc.page_content, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dfb981",
   "metadata": {},
   "source": [
    "# Make the code more modualar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea6c8f",
   "metadata": {},
   "source": [
    "### Create a function that will check last modified time of the files and if it is not new we won't need to re-create the vector store\n",
    "Below there are functions that are included in ```rag.py``` - they check for the vector store and if it is up to date with pdf documents in a given folder. If no, the store is recreated. Furthermore, there is a processing of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e10ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "from datetime import datetime\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env\n",
    "\n",
    "class PDFProcessor:\n",
    "    \"\"\"\n",
    "    Processes PDF documents:\n",
    "    - Extracts raw text\n",
    "    - Splits text into sections based on headings\n",
    "    - Cleans text for NLP\n",
    "    - Splits text into chunks with metadata\n",
    "    \"\"\"\n",
    "    def __init__(self, pdf_folder: str, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "        self.pdf_folder = pdf_folder\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap\n",
    "        )\n",
    "        self.files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "    def extract_text(self, file_path: str) -> str:\n",
    "        \"\"\"Extracts raw text from a PDF using PyPDF2.\"\"\"\n",
    "        text = \"\"\n",
    "        reader = PdfReader(file_path)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "        return text\n",
    "\n",
    "    def split_into_sections(self, text: str) -> dict:\n",
    "        \"\"\"Splits text into sections based on detected headings.\"\"\"\n",
    "        lines = text.splitlines()\n",
    "        sections = {}\n",
    "        current_heading = \"Document\"\n",
    "        current_content = []\n",
    "\n",
    "        for line in lines:\n",
    "            stripped = line.strip()\n",
    "            if stripped and len(stripped.split()) <= 6 and stripped[0].isupper() and not stripped.endswith(\".\"):\n",
    "                if current_content:\n",
    "                    sections[current_heading] = \" \".join(current_content).strip()\n",
    "                current_heading = stripped\n",
    "                current_content = []\n",
    "            else:\n",
    "                current_content.append(stripped)\n",
    "\n",
    "        if current_content:\n",
    "            sections[current_heading] = \" \".join(current_content).strip()\n",
    "\n",
    "        return sections\n",
    "\n",
    "    def clean_text(self, text: str, lowercase: bool = True) -> str:\n",
    "        \"\"\"Cleans text: lowercases, removes punctuation, normalizes spaces.\"\"\"\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text.strip()\n",
    "\n",
    "    def process_pdfs(self):\n",
    "        \"\"\"\n",
    "        Processes all PDFs in the folder:\n",
    "        - Extracts text\n",
    "        - Splits into sections\n",
    "        - Splits sections into chunks\n",
    "        - Returns chunks and metadata\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        all_metadatas = []\n",
    "\n",
    "        for file in self.files:\n",
    "            pdf_path = os.path.join(self.pdf_folder, file)\n",
    "            raw_text = self.extract_text(pdf_path)\n",
    "            sections = self.split_into_sections(raw_text)\n",
    "\n",
    "            for section_title, content in sections.items():\n",
    "                cleaned_content = self.clean_text(content)\n",
    "                chunks = self.text_splitter.split_text(cleaned_content)\n",
    "                all_chunks.extend(chunks)\n",
    "                all_metadatas.extend([{\"source\": file, \"section\": section_title}] * len(chunks))\n",
    "\n",
    "        return all_chunks, all_metadatas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31853c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag.py\n",
    "# Contains PDFProcessor and VectorStoreManager definitions\n",
    "\n",
    "# main.py\n",
    "from rag import PDFProcessor, VectorStoreManager\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PDF_FOLDER = \"assets/documents\"\n",
    "PERSIST_DIR = \"./persist\"\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "processor = PDFProcessor(pdf_folder=PDF_FOLDER)\n",
    "chunks, metadatas = processor.process_pdfs()\n",
    "\n",
    "manager = VectorStoreManager(\n",
    "    pdf_folder=PDF_FOLDER,\n",
    "    persist_dir=PERSIST_DIR,\n",
    "    embeddings=embeddings,\n",
    "    chunks=chunks,\n",
    "    metadatas=metadatas\n",
    ")\n",
    "\n",
    "vectorstore = manager.load_or_create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764622f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchHelper:\n",
    "    def __init__(self, vectorstore):\n",
    "        self.vectorstore = vectorstore\n",
    "\n",
    "    def search(self, query: str, method: str = \"similarity\", k: int = 3, expanded_terms=None, filter_dict=None):\n",
    "        \"\"\"\n",
    "        Runs different search strategies on the vectorstore.\n",
    "        \n",
    "        Args:\n",
    "            query (str): main query\n",
    "            method (str): one of [\"similarity\", \"similarity_score\", \"mmr\", \"expansion\", \"filter\"]\n",
    "            k (int): number of results to return\n",
    "            expanded_terms (list[str]): optional extra terms for expansion\n",
    "            filter_dict (dict): optional metadata filter\n",
    "\n",
    "        Returns:\n",
    "            list of documents (or docs+scores if method=\"similarity_score\")\n",
    "        \"\"\"\n",
    "        if method == \"similarity\":\n",
    "            return self.vectorstore.similarity_search(query, k=k)\n",
    "\n",
    "        elif method == \"similarity_score\":\n",
    "            return self.vectorstore.similarity_search_with_score(query, k=k)\n",
    "\n",
    "        elif method == \"mmr\":\n",
    "            return self.vectorstore.max_marginal_relevance_search(query, k=k, fetch_k=15)\n",
    "\n",
    "        elif method == \"expansion\":\n",
    "            terms = [query] + (expanded_terms or [])\n",
    "            all_results = []\n",
    "            for term in terms:\n",
    "                all_results.extend(self.vectorstore.similarity_search(term, k=k))\n",
    "            # Deduplicate by (source, content)\n",
    "            unique_results = { (doc.metadata['source'], doc.page_content): doc for doc in all_results }\n",
    "            return list(unique_results.values())\n",
    "\n",
    "        elif method == \"filter\":\n",
    "            return self.vectorstore.similarity_search(query, k=k, filter=filter_dict or {})\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown search method: {method}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def pretty_print(results, with_score=False):\n",
    "        \"\"\"Helper to print results cleanly.\"\"\"\n",
    "        for i, item in enumerate(results):\n",
    "            if with_score:\n",
    "                doc, score = item\n",
    "                print(f\"\\n{i+1}. {doc.metadata.get('source','?')} - {doc.metadata.get('section','?')} (score={score:.4f})\")\n",
    "                print(doc.page_content[:300], \"...\\n\")\n",
    "            else:\n",
    "                doc = item\n",
    "                print(f\"\\n{i+1}. {doc.metadata.get('source','?')} - {doc.metadata.get('section','?')}\")\n",
    "                print(doc.page_content[:300], \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4700a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = SearchHelper(vectorstore)\n",
    "\n",
    "# 1. Simple similarity\n",
    "results = searcher.search(\"Maternity leave policy\", method=\"similarity\", k=3)\n",
    "searcher.pretty_print(results)\n",
    "\n",
    "# 2. Similarity with score\n",
    "results = searcher.search(\"Maternity leave policy\", method=\"similarity_score\", k=5)\n",
    "searcher.pretty_print(results, with_score=True)\n",
    "\n",
    "# 3. MMR\n",
    "results = searcher.search(\"Maternity leave policy\", method=\"mmr\", k=5)\n",
    "searcher.pretty_print(results)\n",
    "\n",
    "# 4. Query expansion\n",
    "results = searcher.search(\"Maternity leave policy\", method=\"expansion\", expanded_terms=[\"parental leave\", \"pregnancy leave\"])\n",
    "searcher.pretty_print(results)\n",
    "\n",
    "# 5. With metadata filter\n",
    "results = searcher.search(\"Maternity leave policy\", method=\"filter\", filter_dict={\"section\": \"HR Policies\"})\n",
    "searcher.pretty_print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5652c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def expand_query(query: str, n_terms: int = 5) -> list[str]:\n",
    "        \"\"\"\n",
    "        Use LLM to generate related terms for query expansion.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Generate {n_terms} synonyms of the core word/phrase of the following query for use in document retrieval. Keep them short, noun-phrases.\n",
    "\n",
    "        Query: \"{query}\"\n",
    "        \"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\":\"user\",\"content\": prompt}],\n",
    "            max_tokens=100\n",
    "        )\n",
    "        text = response.choices[0].message.content.strip()\n",
    "        return [t.strip(\"-â¢ \") for t in text.split(\"\\n\") if t.strip()]\n",
    "exp_terms = expand_query(\"Maternity leave policy\")\n",
    "results = searcher.search(\"Maternity leave policy\", method=\"expansion\", expanded_terms=exp_terms)\n",
    "searcher.pretty_print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
